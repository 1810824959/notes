<h1>7.1</h1>

**1.短网址原理**
<ul>1.定义：类似于微博的那种 https://t.vn/xxxxxxxxxxx</ul>
<ul>2.流程：</ul>

	1.浏览器输入https://t.vn/xxxxxx
	2.dns获取ip地址
	3.向ip地址请求，服务器收到 路径xxxxx，进行查询，对应到相应的长网址
	4.用301的方式重定向

<ul>3.算法</ul>

	###要求
	1.每个长网址对应的短网址，要独一无二
	2.短网址要短
	3.不能是随机的，一个长的只能对应一个短的，相同的长网址进来也能是同一个短网址

	###自增序列算法（永不重复算法）
	利用mysql的id自增来实现，可以保证每个都重复
	并把id值，通过进制转换，变成短字符串，比如36进制 [0-9a-z]，也可以更多 [a-zA-Z0-9] 等等，总之1亿在36进制也就6位而已

	###注意点
	1.长链接申请创建时，先查查有没有，这样就解决了相同的问题，但是长字段查起来有太慢，可以对其做hash
	2.模拟哈希索引

	   新建一列用于存储该字符列的hash值（哈希函数不要使用SHA1(),MD5(),因为会产生很长的字符串，浪费空间，比较也慢，最好是返回整数的hash函数），在该列建立索引，查询时必须在where子句中包含常量值，以避免hash冲突
	
	      例如： SELECT  id FROM url
			    WHERE url_hash = hash('www.blog.csdn.net')
	            AND url = 'www.blog.csdn.net'

<ul>4.自定义短码的处理</ul>

	###自定义出现的问题
	用户自定义了一个，占了一个id的位子，那么这个id就被浪费了
	且如果由其他id生成的短码，已经被自定义过了呢

	###解决
	id生成的，发现被自定义过了，就把这个自定义的行对应的id拿过来，反正这个id空着的，用这个id去生成，再被占用就继续，反正位子多
<ul>链接：https://www.jianshu.com/p/43eea66a2235</ul>



**2.杂**
<ul>hash的引用场景</ul>
<ul>更加细化的去重，比如同一域名下的很多网址，全部拿来去重很没效率</ul>
<ul>生成唯一id</ul>
<ul>SHA1(),MD5()??返回整数的hash函数？</ul>
<ul>手写多线程</ul>
<ul>互娱和雷火都开始了，准备投，还有阿里</ul>

**3.TopK问题**
<ul>1.问题描述：从一堆数中找出最大或者最小的 K 个数：相应的，数据量会非常大，先排序再找，不现实</ul>
<ul>2.思路：用堆来做，在一开始，先创建大小为K的堆，对其进行初始化，如果是最大堆，顶上就是最大值，这时候遍历集合，和最大值比较，比最大值小就替换，重新调整，这样遍历完堆里就是最小的K个值</ul>
<ul>3.Java实现</ul>

	public static int[] topK(int [] nums , int k){
        if (k>=nums.length){return nums;}
        int[] res = new int[k];
        for (int i=0;i<k;i++){   //创建一个堆
            res[i]=nums[i];
        }
        for (int j = (k-1)/2 ; j>=0 ; j--){   //初始化这个堆
            shift(res,j);
        }
        for(int item : nums){
            if (item<res[0]){
                res[0]=item;
                shift(res,0);
            }
        }
        return res;
    }

    public static void shift(int [] nums , int index){
        int i = index;
        int j = 2*i+1;
        while (j<nums.length){
            if (j+1 < nums.length && nums[j+1]>nums[j]){
                j++;
            }
            if (nums[i]<nums[j]){
                int temp = nums[i];
                nums[i] = nums[j];
                nums[j] = temp;
                i = j;
                j = 2*i+1;
            }else {
                break;
            }
        }
    }


<h1>7.3</h1>

**1.算法练习**
<ul>见 算法.md</ul>

**2.杂**
<ul>1.ArrayBlockingQueue 这里的公平锁，本质上，就是重入锁的公平锁，就是在释放锁的时候，下一个拿到的是不是符合公平原则</ul>


<h1>7.5</h1>

**1.算法练习**
<ul>见 算法.md</ul>

**2.杂**
<ul>1.BFS就是广度优先遍历，一般使用队列来实现，DFS就是深度优先遍历，如果用数据结构写，就是栈，如果不是，一般就是回溯法</ul>
<ul>2.最短路径算法</ul>

	1.之前BFS用队列来做，是因为每个点之间的距离权重是一样的，此时加了权重，为保证最小所以需要用优先级队列，最小的在前
	2.狄杰斯特拉算法：主要是维护两个东西：1.从起点到终点的中间每个点的距离   2.每个点最短路径中的，上一个点  3.还有一个辅助的数组，表示哪些已经被搜索过了

	###大致思想
	首先从起点开始，写下起点到它能到的所有点的距离，到不了的就无穷大，把这个起点放进seen里面，表示已经搜索过了。

	下一步，从其余点中找出距离最小的点（注意，这个cost队列，存的都是到原点的距离），把最小的点作为起点，重复一遍，新距离是起点到该点的最短距离+到下一个点的距离，如果够小，就更新

	直到所有的都结束，都放进了seen为止

<h1>7.7</h1>

**1.倒排索引**
<ul>1.定义：顾名思义，和正向索引不同，是用value来索引key</ul>
<ul>2.应用场景：搜索引擎，Mysql全文索引等等</ul>
<ul>3.具体：</ul>

	比如，百度搜索“华为”，我要去找每一个网页，看看里面有没有华为两个字，找出来之后还要进行打分，给他排序，肯定是不行的，所以事先维护一个由“华为”字符做key，value是哪些文档含有它

	需要一个单词列表，每一项是包含的文档，再细致一点，可能还把频率，等等加进去

	单词列表的查询，可以像hashmap一样用数组加链表，也可以直接用树，还有在建立倒排索引之前，要先对文档进行分词，把单词找出来

	查询一个组合的字符，就对应他们的交集

**2.MySQL读写分离和主从复制**
<ul>1.关系</ul>

	主从复制是读写分离的前提

	有时候数据库压力很大，就可以考虑读写分离，一个数据库负责写，一个负责读，就可以提升一倍性能，但是为了保证两个数据库数据要一样，就必须用主从复制

	
<ul>2.主从复制</ul>

	1.主库要开启binlog的日志功能
	2.不同库的id要不一样
	3.保证连通

	主库做的每个操作，每个sql语句，都记录在binlog的二进制文件里，每当一个从库连接进来，主库就会新开一个线程，传到从库，从库这时候有两个线程，一个接受二进制并存到relay.log，还有一个，去串行执行语句

	###主库宕机？怎么保证数据不丢失？
	采用半同步复制，写进binlog的同时，本来是异步复制，不用管从库，但是现在不行，必须确保有一个从库接收到了，才会显示commit成功，保证了不管主库宕不宕机，总是有一份数据是完整的

	之所以是叫半同步，是因为当等待时间超过一定设置值，主库就不管他了，退化成异步复制

	###从库是一条一条来的，太慢数据延迟咋办
	有一个并行复制，是从库的relay进行并行，5.7版本新增，主库怎么并行执行，从库就怎么并行
<ul>3.读写分离</ul>

	一台服务器负责insert,update操作，另一台负责select，前提是两者之间数据同步

	1.代码上实现，插入更新操作发到写服务器，查询操作发到读服务器。
	2.通过数据库中间件（mycat,阿里的Cobar）

**3.mysql分区分表**
<ul>1.水平拆分：比如按年查询的，就把今年的分出来，因为被查的几率大嘛</ul>
<ul>2.垂直拆分，一个表的字段，可以拆成多个表，在查的时候，单独去查，代码层拼起来就好</ul>
<ul>3.垂直拆分可能会有分布式事务的问题，就按照分布式锁来</ul>


<h1>7.8</h1>

**1.分布式ID生成器**
<ul>1.要求：</ul>

	1.肯定要不一样啊
	2.最好是递增，方便查询
	3.高可用，单点挂了可不行
	4.ID要短一点，方便存，占用空间不能太大
	5.批量生成

<ul>2.生成方式</ul>
	
	##snowflake
	一个id有64bit
	首先第1bit空着，为0
	2——41bit是当前的时间戳到某个时间的毫秒
	后面的10bit是机器id，这样就是2的10次，可以有1024台机器
	最后12bit，就是代表，该机器当前单位时间内（毫秒），能生成的数量，这里是4096，那样一秒就是4096000，QPS很大了

	实际上，上面的东西可以进行调整

	###其他方法
	1.用mysql主键，但是单机版的可能不太行，主从的话，还要考虑主从复制
	2.UUID，这样就不保证递增（随机）
	3.直接根据当前时间来，这样就不能批量了
<ul>3.链接：https://www.jianshu.com/p/2e57acbe4a19，下方的refer也很好</ul>


2.**redis操作**
<ul>1.String</ul>

	1.set key value / get key / del key
	2.incr/decr key : 自增或自减
	3.incrBy/decrBy key num :自增或自减某个数
	4.append ：加在最后
	5.getRange key start end(后包括) / setRange key start offset
	*6.setnx key value
	7.mset key1 value1 key2 value2/这个也有setnx版本

<ul>2.list</ul>

	1.LPUSH/RPUSH key value [value ...]
	2.LPUSHX/RPUSHX当key存在时才插入
	3.LRANGE key 0 -1
	4.LINDEX key index/LSET key index
	5.LREM key count value
		count > 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count
		count < 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值
		count = 0 : 移除表中所有与 value 相等的值
	6.BLPOP/BRPOP：阻塞操作，并有timeout
	
![](img/2019-7/Hash.png)
![](img/2019-7/List.png)
![](img/2019-7/Set.png)
![](img/2019-7/String.png)
![](img/2019-7/Zset.png)

**3.redis的底层**
<ul>1.String：String可以存整数，可以存动态字符串，关于字符串，类似于数组，叫做SDS，里面主要三个变量，字符长度len,尚未使用的buff缓冲区长度，还有字符数组，所以相应的，len操作就是O(1)</ul>
<ul>2.List</ul>

	3.2版本以前的老版本，是ziplist和linkedList混用的，当元素少于512且都小于64字节，用ziplist，不然就转码成Linkedlist，新版本用的是quickList，是两种的混合，大层面是linkedlist,但是其中的每一段是ziplist

	##ziplist
	本来一个链表，需要一个节点，然后用前后指针连接起来，浪费内存空间，现在直接用一块完整连续的内存，来做这件事情
	###linkedlist
	双向链表

<ul>3.Hash</ul>

	当元素少于512，且都小于64字节，就用ziplist,不然就用hashtable，这里的hashtable，和java的hashmap差不多，用的是数组加链表.

	而这里的当超过负载因子，也需要扩容，或者缩容，然后rehash。但是这里不是一次性就rehash，采用渐进式的。

	假设元素很少，一次性弄完就好，假设元素很多，就很耗时了，而且redis是单线程，会卡住。所以，当要扩容时，先申请数组，但不立马搬移，当有修改，插入的时候，就插入在新数组，同时从老数组搬运一个到新数组，这样其实同时有两个数组在工作，当全部搬移成功，老数组就淘汰了，这就是渐进式rehash
<ul>4.Set</ul>
	
	主要是intset和hashtable

	当集合里都是整数，而且元素不多，就用intset，不然用hashtable，set这里用hashtable是可以的，只不过value都是null
<ul>5.Zset</ul>

	ziplist和skiplist（跳跃表）

	###跳跃表
	一开始是一个head头结点，头指向跳跃表的头，尾指向跳跃表尾巴。最开始有一个节点，上面每个元素都有，可以理解为权值为0，每一个元素都有前进指针和后退指针，同时伴随着step，比如，向前一步，就到了权值为1的节点上，所以，相应的，每个权值的节点，都是有每个元素的

	###
	跳跃表查询删除平均o(logn)，最坏N

	链接：https://blog.csdn.net/universe_ant/article/details/51134020
<ul>用途</ul>

	String：缓存、限流、计数器、分布式锁、分布式Session
	Hash：存储用户信息、用户主页访问量、组合查询
	List：微博关注人时间轴列表、简单队列
	Set：赞、踩、标签、好友关系
	Zset：排行榜


<h1>7.9</h1>

1.Redis 与 Memcached

<ul>1.数据类型:Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型，可以更灵活地解决问题。</ul>
<ul>2.数据持久化:redis可以，memcached不可以</ul>
<ul>3.Memcached 不支持分布式，redis可以做集群</ul>
<ul>4.内存管理机制：在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。</ul>

2.抽象类与接口的区别
<ul>1.定义上：接口是public interface ，里面只有方法名，抽象类是定义时加上abstract，里面可以有不是抽象的方法（注意，抽象类可以有私有变量和私有方法，而且抽象方法只有名字）</ul>
<ul>2.性质：都不能被直接实例化，一个普通类可以继承多个接口，但只能实现一个抽象类</ul>
<ul>3.构造方法：接口里面的变量都是static final的，不能有构造方法，也不需要，因为构造方法的初始化顺序在最后，而抽象类就可以有构造方法，因为除了抽象方法，另外没有区别</ul>

<h1>7.10</h1>

**1.hashmap为什么容量都是2的幂**
<ul>在计算数组下标时，调用indexFor方法，里面是 hash&(length-1) ,当扩容前容量是16，len-1就是01111，扩容后容量是32,len-1就是011111 ，仅仅是多了一个1，那么在 & 的时候，只要多出来的那一位是0，下表就是不变的，是1，那就是之前的两倍，rehash就会省下很多计算消耗</ul>

**2.hashmap为什么链表大于8，就变成树，小于6就退化成链表**
<ul>红黑树的查找长度是 logn ,log8=3 ，而链表的平均查找长度是 0.5length , 在这里就是4，所以优化，为什么不是7呢？因为7+1就是8，7-1就是6，太容易波动</ul>

**3.hashmap jdk7 rehash死循环**
<ul>1.起因：jdk7用的是头插法，不仅仅是扩容是头插，平时插入元素也是头插</ul>
<ul>2.</ul>
<ul></ul>


4.分布式学习
<ul>1.分布式锁</ul>
	
	1.基于mysql主键，但是没有超时的限制，而且不可实现重入锁
	2.redis setnx ，set if not exist，当链接断开时，因为有expire的存在，可以超时删除，而且string可以实现自增，就可以实现重入锁。唯一的问题是，当A超时，B,C检测到，同时去删除A并更新自己的，C可能删除掉B刚才更新的，那样B,C都有锁了。这个问题，当检测到超时，第一时间先别删，先去getstat一下，如果发现不是A，就不删除了，因为锁已经被人拿走了
	3.zookeeper,在某一目录下创建临时节点，比如在 /lockxai 创建，因为节点会有自增的序号，这样如果是最小的序号，就说明拿到锁了,没拿到的监听自己前一个节点，前一个被删除了，自己就是了。至于可重入，在代码端设置一个state就可以实现可重入了

<ul>2.分布式事务</ul>
	
	###CAP
	一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），一般只能实现两个

	###BASE
	基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）
	基本可用很容易理解，软状态就是允许不同系统间存在中间状态，允许数据在同步的过程中有一定延迟，最终一致性就是，只要保证最终是数据一致的就可以

	###2pc(两阶段提交)
	设置一个协调者，和其他的参与者，协调者发出执行事务的命令，让每个参与者都去执行，但是都不提交，每个参与者会把结果返回给协调者，如果每个参与者都OK，协调者就发送commit的提交命令，让每个参与者都提交，一旦有一个不行，就全部回滚

	缺点：1.参与者在等其他参与者确认的时候，有很大延时。  2.协调者是单点的，故障怎么办？  3.发出去的命令是可能丢失的，有的参与者没收到，就会有数据不一致性   4.机制太保守，一个不行，就全部回滚

	###消息中间件做分布式事务

	###3pc（三阶段提交）
	在最前面加上了一个询问的过程，协调者询问参与者，参与者预估一下能不能成功，并返回。这一步增加了一次检查，避免了许多不能成功提交的情况。再者，在协调者和参与者上都加上timeout，超时就默认提交，这是根据概率学来的，基本都是成功的


<h1>7.11</h1>

1.常见笔试题

2.paxos算法，还有他和zab的关系
<ul>1.流程</ul>

	首先有三个角色：Proposer(提案者)，Acceptor(决策者),learner（学习者）。首先Proposer（不管多少个）先生成自己的提案id，注意，每个人的提案id都是唯一且自增的。如果最开始没有现成提案内容，就用自己的
	
	现在，提案者发送提案给所有决策者。决策者有着两条原则：
	1.不接受比已持有提案id还要小的提案，也就是，我收到过6的，你给我个5，我就不理你 
	2.如果决定接受，就返回已经拿到过的最大提案的内容

	现在终于开始讲解流程
	首先，决策者先进行试探(prepare)，发送一个带着提案id的空提案给超过半数的Acceptor，两个目的，第一，确保半数的已经准备好了，第二，如果已经有批准的提案，学习之，因为Acceptor会返回。在这里，如果收到了半数确认，就进入下一阶段，发送正式内容，如果没有，就增加id，再来一次，直到ok为止

	经过一系列，终于要正式发送了。前面Acceptor没回复值的，就发送自己的内容，有回复值，就发送回复值，如果有半数的通过，就交给学习者，已经Ok了

	既然已经有了正式的通过提案，那么整个系统就都要学到。已经有一个通过了半数，那说明另外的都没有通过，都会被赶回去重新进行prepare阶段，他们再次发送，会得到已经通过的提案内容，因为Acceptor已经有了他们，这样，各个propsoer也会学到这个值

<ul>2.zab是它的变种</ul>




**3.redis秒杀场景设计**
<ul>1.要解决的难点：1.高并发的数据库压力和网络负载  2.实现正确的购买顺序，解决超卖等问题</ul>
<ul>2.高并发压力</ul>

	##阻挡掉大部分流量
	1.比如抢购场景，用户点一下卡住了，喜欢多点几下，这就造成了大量冗余，不必要的申请，这里可以在用户点击一次之后，就将按钮设置成不可点击，或者直接设置成一段时间内，用户只能发起一次请求

	2.还有不怀好意的脚本，绕过了按钮，直接访问接口，商城对每一个用户肯定有单独的userID，一段时间内，对同一id返回同样的内容，或者直接静态缓存页面，每次都返回缓存，拦截多余流量

	##设置redis缓存，减少数据库压力
<ul>2.超卖问题</ul>

	高并发下，两个线程同时查到还剩最后一件，两个线程都进行-1操作，就出现了超卖，这里可以使用redis，redis自身操作就有原子性。假设有1000件商品，那从一开始就设置一个List，里面是啥无所谓，反正每次进来都lpop一次，对用户而言，只有lpop成功，才显示成功，避免了超卖

	###异步操作
	而到了这里，就不用急了，把这些得到允许的操作，做成队列，让mysql慢慢来就行，不一定要追求很高的同步性

	###事务watch
	或者可以用redis事务中的watch来实现，监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断，实际上就是乐观锁了


<h1>7.12</h1>

1.**手写实现读写锁**
<ul>1.读写锁概念:对资源的访问，分为读锁和写锁，读操作可以共享，但是读的时候，不能被写，写的时候，又不能读，又不能写。所以总结就是，读锁上只可以加读锁，写锁不能加任何锁</ul>
<ul>2.wait() / notify()</ul>

	1.概述：这两个是object()上的方法，而且是final的，那也就是不能被重写
	2.wait():当已经获取当前对象锁时，让当前线程阻塞，变成wait状态，时刻等待唤醒，这里有两个注意点：一，前提是要先有锁，二，这里直接出让锁，Thread.sleep是睡眠，实际不出让锁
	3.notify和notifyAll：去唤醒正在等待的锁，实际谁拿到还说不定呢

<ul>3.实现</ul>

	private  int readCount = 0;
    private  int writeCount = 0;
    public void lockRead() throws InterruptedException {
        while (writeCount>0){
            synchronized (this){
                wait();
            }
        }
        readCount++;
        System.out.println("read lock");
    }

    public void unlockRead(){
        readCount--;
        synchronized (this){
            notifyAll();
        }
    }

    public void lockWrite() throws InterruptedException {
        while (writeCount>0){
            synchronized (this){
                wait();
            }
        }
        writeCount++; 
        while (readCount>0){
            synchronized (this){
                wait();
            }
        }
        System.out.println("write lock");
    }
    public void unlockWrite(){
        writeCount--;
        synchronized (this){
            notifyAll();
        }
    }

	###解决一直读累加问题
	在写锁方法里，提前writeCount++
	这里面其实都很巧妙
	while (writeCount>0){
            synchronized (this){
                wait();
            }
    }
	当拿不到锁，会一直循环，当拿到锁，又会出让，其实就是想等到跳出循环的那一刻

<ul>4.链接：https://www.cnblogs.com/DarrenChan/p/8619476.html</ul>





**2.一致性hash（今天面试问到了，学习一下）**
<ul>1.存在的意义，为了解决普通hash算法在分布式中的缺点</ul>
<ul>2.应用场景</ul>

	###
	比如，将图片缓存进redis，单机还没事，当因为性能，设计成集群，主从结构的时候，就问出现问题。当数据一大，就会分库分表，同一类的缓存可能会存在不用的redis上的，要用的时候，每台服务器去找肯定不现实，所以在这里用到hash算法，O(1)的复杂度就定位到相应的服务器

	但是，假设现在有5台，hash函数 xx%5 ，数据被存在其中一个了，这时候撤掉一台机器，那每个缓存是不是都要重新rehash一下？？？这时候，就出现了一致性hash，让机器数量虽然改变，大部分的缓存还是不用变位置

<ul>3.原理</ul>

	简单的hash函数，使用机器数量做除数来取余，现在进行修改，使用 2^32作为除数，让结果落在 0~2^32-1上，把这些下标抽象在一个环上，按照顺时针大小递增的一个环，每台机器的hash分别落在环的不同位置。

	现在，当每一个缓存进行hash，肯定也是落在环上，那么从落脚点往顺时针走，最先碰到的服务器就是他缓存的地方
<ul>4.优点</ul>

	1.使用定值2^32做除数，而不是机器数量，当机器数量改变时，大部分数据的hash不需要更改

	2.当机器数量更改，变动的只有一小段的数据，这一段会定位到新的服务器，仅此而已
<ul>5.细节上的小问题</ul>

	##为什么除数是2^32
	考虑到，服务器都是要hash到环上的，肯定不能有重复啊，而且考虑到扩展，要留下足够的空间，而java中int最大就是2^32，一个int是4个字节，一个字节8位，那当然是32位

	##hash倾斜的处理
	服务器比较少，而且hash出来很近，这就导致后面一个节点，只能被上一个到它自己这一段很小的距离内的数据找到，其他大部分都到了另一个服务器上，毕竟是顺时针找嘛。

	这种时候，创建好几个虚拟节点，故意弄不同的值，让hash出来的点稍微均匀一点，让其中的交叉排布的若干点指向A，另外指向B，这样即使数量少，也会比较均匀

	##链接
	https://www.cnblogs.com/study-everyday/p/8629100.html#autoid-2-0-0
<ul></ul>

3.杂
<ul>1.hash出来的都是整数？？？hash具体算法有哪些？</ul>
<ul>2.redis 事务，watch??和乐观锁啥关系</ul>
<ul>3.接到offer就减肥！！！！！还有尽量早睡</ul>


<h1>7.13</h1>

1.杂
<ul>1.幂等性？？</ul>
<ul>2.select xxxxxxxxx for update，查询语句中涉及主键，就锁行，没有就锁表，直到commit提交事务才会被释放</ul>


<h1>7.15</h1>

1.下阶段目标
<ul>1.算法</ul>
<ul>2.dubbo</ul>
<ul>3.复习之前的</ul>

2.动态规划再学习
<ul>1.上次没做出的题目：最长递增序列</ul>

	###当时的想法
	一直纠结于一个元素，选或者不选。并尝试写一下方程，是F[n] = max(F[n-1],F[n-1]+1)，到了这里就卡住了，实在下不去了

	###正确思路
	其实这里应该是发挥，动态规划一个类似于数组的做法，新增一个数组，第N个元素表示，原序列前N个里的最大递增序列的长度。每当一个新的元素加进来，就遍历比较0~N-1的元素，找到符合递增的，最大的长度值，+1就好
<ul>2.经典的01背包问题</ul>

	首先想想能不能写出方程，发现写的出，F[i][j] = max(F[i-1][j],F[i-1][j-weight[i]+value[i])，意思就是说，第i个，不选就是沿袭i-1的值，选，就是i-weight的值加上第i个本身的价值
<ul>3.自己的总结：首先尝试写写状态方程，写不出别急，想想数组这种的行不行</ul>
<ul>4.最大子序列和</ul>

	这个问题老是考察，又老是忘记

	首先写下状态方程，一般是这么写，令f(n)代表前n个数中的最大序列和，如果前面的和小于0，那就不要了，直接是arr[n]，如果大于0，就是f(n-1)+arr[n]，按照道理是应该这么写，但是这里用f(n)不太合适，因为因为如果最大的子序列断了，就应该是新的了，不能再用f(n-1)代表之前的最大值，应该用sum这种，如果小于0，就置为0，并且每次都和max比较

	
<h1>7.16</h1>

1.二叉树（里面的有些递归，不太好写）
<ul>1.二叉搜索树的公共祖先</ul>

	说了是二叉搜索树，那就遵循左小右大。

	思路：拿到一个节点，进行判断，如果要找的两个点都在左边，说明都在左子树，那就用该节点的左节点，再来一次，右边的一样。结束的条件是，一个在左，一个在右，或者干脆和其中一个相等，这样，该节点就是最近公共父节点

<ul>1.普通二叉树（待补充）</ul>

2.netty和nio
<ul>1.传统io：基于字节流，是一个字节一个字节来的，但是NIO是根据块传输的，是一块一块来的</ul>
<ul>2.nio的本质：nio也叫new io，实际上是基于reactor模式的，多路复用的非阻塞IO</ul>
<ul>3.netty的流程：一般的服务端过程，创建两个netty自带的线程池，一个Boss，一个worder，boss用来accept请求，worker用来处理，随后创建serverboostrap启动类，在里面指定channel和对应的handler。这里的handler一般是一个pipeline一样的东西，里面add上实现的handler。具体的话，就是去重写里面的channelRead0方法，最里面最重要的就是bytebuf这个对象</ul>

3.粘包
<ul>1.粘包的定义：其实也就是包在被接受的时候，多个包黏在一起，或者两个包各一半，粘在一起，分不出来，本质上是包的边界问题</ul>
<ul>2.粘包出现的原因</ul>

	###协议设计层面(Nagle)
	tcp是基于流传输的，这样就是根据缓冲区大小来传输，而且tcp有一个nagle算法，会合并许多小的包，只进行一次传输，来提高利用率，不然会有许多很小的包，却占用了大量传输资源。但是，这样就会造成多个包粘在一起的情况，就是粘包

	###发送端
	不足缓冲区，就会有nagle算法，超过缓冲区，就会分成多个包
	###接收端
	接受的太慢，一个包还在缓冲区，另一个也来了

	###另外层面(消息边界)
	消息边界，就是保证了，一个包，就是一条消息，肯定会被接收端进行一次接收，不存在多个包一起的情况，所以udp就没有粘包问题，而tcp是流传输，就是没有消息边界的意思
<ul>3.怎么解决</ul>

	1.从协议角度，直接禁用Nalge算法

	2.使用场景角度，只做一锤子买卖，发送的只发一次，接受的也只接受一次，直接立马关，下次要用，重新建立连接

	3.编程角度：使用定长数据包，或者用特殊字符做边界符，也可以使用“长度+内容”的形式，不然的话tcp每次都强制发送，不等缓冲区

	4.接收端角度：优化消息处理速度，也就是处理的快一点，不要老是堆在缓冲区

4.算法
<ul>1.最长回文子串</ul>

<ul>2.判断是否回文串</ul>

<h1>7.17</h1>

**1.Reactor模式**
<ul>1.Reactor模式</ul>

	Reactor模式就是专门为了处理高性能并发的，有这么几个部分
![](img/2019-7/reactor.png)

	Handle:就是要被监听的对象，比如网络编程里的Socket对象，放在Java NIO里就是Channel，也就是请求触发的地方
	Synchronous Event Demultiplexer：也就是接受请求的地方，放在其他地方就是linux的 select函数，Java里的Selector模型
	Initiation Dispatcher：管理执行Handler的东西，对应netty就是NioEventLoop
	Event Handler：那就是netty里的handler了

	###总结
	说句简单的，来了很多请求，先经过selector,然后下达给dispacth进行分发，发给相应的handler。就这么简单，具体还有其他细节
<ul>2.reactor的三种模型</ul>

	###单reactor线程模型
	reactor是单线程的，handler的处理过程也在线程里，一个太慢，会影响其他的

	###单reactor多线程模型
	reactor还是单线程，但是现在，下发到handler的时候，除了read/send之类的io请求，其他的逻辑，统统交给其他的线程。这里用了线程池

	###多reactor多线程模型（netty就是这样的）
	reactor单线程总是有性能瓶颈，所以这里干脆按照功能分开。分为mainReactor和subReactor，main只负责accept请求，下发到相应的sub上，sub再进行dispatch
<ul></ul>

**2.netty和reactor**
<ul>1.对应</ul>

	netty其实就是实现了reactor的多reactor多线程模型
	一般使用的bossEventLoop就是mainReactor，workerEventLoop就是sub的，然后经过ServerBootstrap的配置，channel就是模型里的handler，也就是socket，再下面有一个channelPiple ，上面可以add很多具体的handler

**3.NIO和netty的相关问题**
<ul>1.AIO/BIO/NIO：AIO是异步的，同步异步区别在弄完之后，服务端会不会通知你，至于NIO要分java中的，还是linux的</ul>
<ul>2.NIO的组成</ul>

	Channel
	Buffer
	Selector
<ul>3.NIO和传统IO的区别</ul>

	###传统IO
	1.基于字节流/字符流，总之基于流，按顺序的，一个字节来读
	2.流是单向的，所以有输入输出流
	3.是阻塞的
	4.没有selector

	###NIO
	1.基于channel和buffer，是按照块来传输
	2.channel是双向的
	3.非阻塞（要么读到当前可读的，要么啥都没）
	4.有selector，实现类似多路复用的机制（channel的rigister方法，参数:selector实例，常量型的事件类型，但是channel必须先设成非阻塞，还有selector的keys方法，返回准备好的channel）
<ul>4.NIO和netty搭建简单服务端</ul>

	

	
<ul>5.netty零拷贝</ul>

	###linux零拷贝和netty零拷贝的区别
	linux就是真的，想从内核区，不经过用户区，直接到网络，而netty这里，已经是在jvm中了，所以不能算，只能算是，减少拷贝的次数，减少拷贝量

	###CompositeByteBuf
	想把两个数据合在一起，新创建一个bytebuf，再添加进去，就拷贝了两次，用CompositeByteBuf，添加进去，实际上是逻辑组合，没有拷贝

	###包装wrap
	想把byte[]数组这种变成bytebuf，拷贝进来太浪费资源，Unpooled.wrappedBuffer方法进行包装，还是共享之前数组的地址，不进行拷贝

	###拆分slice
	这次是拆分，slice(int index, int length)，不copy，相当于共享了同一个bytebuf的不同地址而已

	###FileRegion 
	这里的FileRegion 和nio 里的FileChannel一样，有一个transferTo的方法，直接从一个channel转移到另一个channel，不需要buffer缓冲区

<h1>7.18</h1>

1.**自定义类加载器**
<ul>1.流程：首先继承classloader，实现里面的findclass方法，findclass方法里有defineclass方法</ul>
<ul>2.详解</ul>

	loadclass:一般不重写，逻辑是每次都交给父加载器，如果都找不到，就调用findclass
	findclass:默认为空，子类自己实现，里头是读取类的相关内容，并调用defineclass
	defineclass:把字节码变成class，字节码就是byte[]

2.**用户区和内核区**
<ul>Linux有4G的虚拟地址空间，最高的0-1G作为内核空间，剩下的座位用户空间，因为内核空间共享，所以每个进程都可以说是4G空间，但是用户空间独立，而且不共享</ul>

3.static用法
<ul>1.概述：static，就是没有this的意思，么的实例对象的意思</ul>
<ul>2.修饰方法：static方法，就变成类方法，用类来调用，相应的，因为脱离了对象，所以静态方法里不能调用非静态方法，但是相反，非静态可以调用静态的</ul>
<ul>3.修饰变量：static变量，和上面的一样，是属于类的，但是可以被非静态调用</ul>
<ul>4.静态代码块：在初始化时，按照顺序初始化，但是有一点，静态代码块只会被加载一次</ul>
<ul>5.其他：static的变量虽然是属于类的，但是通过对象，也是可以访问到的</ul>

<h1>7.21</h1>

1.内部类
<ul>1.特点：内部类可以任意访问外部类数据，外部类创建实例，就可以访问任意对象</ul>
<ul></ul>

2.**布隆过滤器(bloom filter)**
<ul>1.作用：判断某个元素是否在集合中，取代了hashmap在大数据量中去重的地位</ul>
<ul>2.原理</ul>

	1.首先是一个bit数组，里面初始化都置为0
	2.同时给出K个不同的hash函数
	3.元素进来时，计算出K个hash值，把下标对应的位置写成1
	4.下次查询时，如果对应位置都是1，就说明有
<ul>3.优缺点</ul>

	###优点
	1.省内存
	2.查询快
	3.查询但是不保存，对数据有安全性
	

	###缺点
	1.有误报可能性(但这里的误报是，说是已存在的，可能不存在，但说是不存在的，就一定是没有，因为查询K个hash值时，可能某个位置被其他的元素已经变成1了)
	2.不支持删除，因为bit位上是0和1，删了一个，其他的相关的元素咋办（或者用计数法，但是就违背了用bit的初衷，节省内存）
<ul>4.其它问题</ul>
	
	###为什么是k个hash函数，一个不是够了吗
	不行。单hash函数，发生冲突的概率很大。有句话很好，想把冲突绿降到1%，就要把bitset大小变成100倍

	###hash函数K值的控制，还有bitset大小的控制
	K越大，覆盖的位数就越多，误报率也就越大，K越小，就越接近单hash冲突。而set长度越大，内存消耗越大，误报率的确会降低，set长度越小，很快的大部分位都会置为1，误报率变大

<ul>5.场景：1.去重(爬虫)  2.用redis做一个缓冲层，先判断有无，就不用每次都查后端数据库，减少数据库压力，但同时，hash函数的效率要保证</ul>

3.**MVCC**
<ul>1.概述：MVCC也就是俗称多版本控制，实现了mysql的读已提交，可重复读。</ul>
<ul>2.作用</ul>

	MVCC和java的copy and write有点像，就是读和写是分开的，分别有多个快照，MVCC也算是乐观锁的体现，不仅仅是mysql，其他数据库也在用，只不过实现细节不太一样

	innodb默认可重复读的级别，用mvcc做到。是因为读未提交，需要每次都是最新的，不需要mvcc来做，串行化又做不到，这个需要锁来实现绝对的串行化
<ul>3.原理：</ul>

	这里以行级为单位，可以说，每一行里面，都会有两个字段，创建的版本号，和删除的版本号，而每一行，都可以有很多份快照，而查询的时候，从来都是查询符合条件的最新快照

	##插入
	就是插入，创建版本号设置为当前的事务id，删除版本号unDefine未定义
	##删除
	这里就是逻辑删除，定义删除版本号为当前事物id
	##更新
	为旧数据行的删除版本号赋值，重新添加一个数据行快照，并定义创建版本号，在这里才会出现同一行数据的不同时期的快照
	##查询
	这是重头戏，查询遵循两个原则
	1.查询版本号小于等于当前事务id(很容易理解吧，版本号大于当前的，要么现在这个时候还没插入，要么是在未来被修改了，总之现在还看不到这个快照)
	2.删除版本号要么没有，要么就是大于当前事务id，否则小于当前的话，说明早就被删了，无效了
	
<ul>4.解决的问题</ul>

	1.实现读已提交：很容易理解，你不提交，哪来的版本号？
	2.实现可重复读：同一事务内，你另一个事务提交的变化，我这里查不到，因为版本号有限制


4.Mysql Group by

5.java可重入锁里的读写锁？？

<h1>7.22</h1>

1.杂
<ul>1.mysql突然打不开：本来我是设置成服务的形式，开启才能用，但是不知道为啥，服务直接消失了</ul>

mysqld --install MySQL --defaults-file="F:\mysql\mysql-5.7.9-winx64\my-default.ini"
<ul></ul>
<ul></ul>

2.java异常机制
<ul>1.关键字：</ul>

	try：里面是可能报异常的代码
	catch：捕获异常，可以有多个catch的代码块，但是只能捕获一个，所以一般小的细的放在前面，大的放在后面
	finally：放在finally中的，保证会被执行。但是有个问题，try中的代码return或者throw咋办，其实是这样，当要返回时，必须要先执行finally代码，才会走到return这一步，但如果finally里头也有return，就直接返回了，不回去上面的return了
	throw,throws：注意喽，throw是主动去抛出，throws是跟在方法后面，声明有这些异常抛出
	
<ul>2.两种异常（还要斟酌一下）</ul>

	###check异常
	编译期异常，这种在编译时就可能报错的，java要求必须要捕获，不然通不过编译（就是除RuntimeException以外的异常）

	###runtime异常
	运行时异常，就算出现这种异常，编译时也看不出来，还是可以编译的，只不过跑起来报错而已（RuntimeException）

	###其他
	RuntimeException一般是程序自身程序的错误，比如空指针，数组越界，除零，类型转换错误，这一类，编译的时候不会被查出来，所以也不用强制去捕获

	IOException这个就是java层面出现的错误，比如IOException：操作输入流和输出流时可能出现的异常，EOFException，文件已结束异常，FileNotFoundException，文件未找到异常
	

	
<ul>3.异常类</ul>
![](img/2019-7/exc.jpg)

	1.Throwable:这些类的父类，只有继承了它，才可以被抛出异常和错误
	2.error：和异常相对，是错误。不被catch捕获，而且程序不处理，一般是java虚拟机本身的错误，比如StackOverflowError最经典栈溢出，还有内存溢出，还有classNotDefineError等，没这个方法，没这个类等等的，也都是错误，是Error
	3.Exception:这就是异常了，这里面又分RuntimeException和IOException。

<ul>4.补充</ul>

	try 块：用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。
	catch 块：用于处理try捕获到的异常。
	finally 块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。在以下4种特殊情况下，finally块不会被执行：
	1）在finally语句块中发生了异常。
	2）在前面的代码中用了System.exit()退出程序。
	3）程序所在的线程死亡。
	4）关闭CPU。

<h1>7.23</h1>

1.编程之美
<ul>1.数组中一个数超过半数</ul>

	这个以前做到过，但是没有很透彻。其实可以这么想，一个数超过半数，现在我删除两个不同的数，要么里面有一个是目标数，要么都不是，但是不管是不是，剩下的数中，目标数还是超过一半

	这样就很容易理解了
	num = null
	count = 0
	for each{
		if i==num ： count++
		else: count--

		如果count为0，遇到新的数，就更新	
	}
<ul>2.题目改改，有3个数，每个都超过四分之一，找出来</ul>

	前面是一个数，一个count，现在弄成一个hashmap，设置三个数和次数，遍历，如果是三个数里的，++，不是，统统减一，当有次数为0 ，就更新



<h1>7.24</h1>

1.一个数出现一次，另外都出现两次
<ul>1.思路：一个数对自己异或就是0，从第一个数开始，遍历异或下来，出现两次的都变成0，只有那个一次的留下来</ul>
<ul>2.题目改改，有两个数出现一次</ul>

	这个麻烦点，首先按照流程走，得到这两个数的异或，这个异或中为1的位，就是这两个数不一样的地方，按照这个位，把整个数组分成两份，再进行就可以


<ul>3.题意改改，一个数组中，少了一个数，怎么找？</ul>

	解法：原来和-现在和
	如果多个一个数？一样
	如果一次性少了两个数？？先是，原来和-现在和，得到丢失两数的和，再同样类似得到两个数的积，就可以列方程算

2.大数乘法
<ul>1.超过int表示范围的数相乘，怎么做</ul>
<ul>2.思路：</ul>

	模拟我们人算来做。每一位的数，分别乘另一数的每一位，相加，就是某一位的数，这时候考虑进位

	两重循环，每一个数去乘，下一层循环的每一个，保存在res的 i+j+1处，因为可能有进位

	def bigNumCh(a,b):
	    lenA = len(a)
	    lenB = len(b)
	    res = [0 for i in range(lenA+lenB)]
	    for i in range(lenA):
	        for j in range(lenB):
	            res[i+j+1]+=a[i]*b[j]  //这里有进位
	
	    for k in range(len(res)-1,0,-1):
	        if res[k]>10:
	            temp = res[k]
	            res[k] = temp%10
	            res[k-1] += temp//10
	    print(res)

	a = [7,7,7]
	b= [8,8,8]
	bigNumCh(a,b)


3.从数组中找出和为K的2个数
<ul>1.解法：双指针法，前提是经过排序，一个指针从头开始，一个从尾开始，相加小于，前面的指针向后一位，大于，后面的向前一位</ul>
<ul>2.改题目，找出和为k的所有组合</ul>

	这样就复杂很多，考虑使用回溯法，一提到回溯法，一般都有队列之类的影子，加入队列进行下一层递归，如果失败就返回来，从队列里拿出来再做

	map = []
	numList = [1,2,3,4,5,6]
	### numList是数组，start是当前加到的位置，sum是已经加上的和，havaList是已经加上的集合
	### 思路：这么一个函数递归，如果加上去超过了，就return退出去，如果ok，就打印，如果小了，就继续下去
	def find(numList,target,start,sum,haveList):
	    path = haveList
	    if sum+numList[start]>target:
	        return
	    elif sum+numList[start]==target:
	        path.append(numList[start])
	        print(path)
	        path.pop()
	        return
	    else:
	        sum+=numList[start]
	        path.append(numList[start])
	        find(numList,target,start+1,sum,path)
	        sum-=numList[start]
	        path.pop()
	        find(numList, target, start + 1, sum, path)
	
	
	find(numList,6,0,0,[])

	
<ul></ul>



<ul></ul>
<ul></ul>
<ul></ul>
<ul></ul>

<ul></ul>
<ul></ul>
<ul></ul>
<ul></ul>

<ul></ul>
<ul></ul>
<ul></ul>
<ul></ul>


<ul></ul>
<ul></ul>
<ul></ul>
<ul></ul>


<ul></ul>
<ul></ul>
<ul></ul>
<ul></ul>